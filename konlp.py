from eunjeon import Mecab
import sentencepiece as spm
from pandas import DataFrame
import pandas as pd

# review = './csv/chatbot.csv'

# df = pd.read_csv(review, sep = ',', encoding= 'utf-8' )
# df1 = pd.read_excel('A ìŒì‹ì (15,726).xlsx', sheet_name='15,726')
# df2 = pd.read_excel('D ì†Œë§¤ì (14,949).xlsx', sheet_name='Sheet1')
# df3 = pd.read_excel('F ì¹´í˜(7,859).xlsx', sheet_name='Sheet1')
# df4 = pd.read_excel('B ì˜ë¥˜(15,826).xlsx', sheet_name='Sheet1')
# df5 = pd.read_excel('C í•™ì›(4,773).xlsx', sheet_name='Sheet1')
# df6 = pd.read_excel('E ìƒí™œì„œë¹„ìŠ¤(11,087).xlsx', sheet_name='Sheet1')
# df7 = pd.read_excel('G ìˆ™ë°•ì—…(7,113).xlsx', sheet_name='Sheet1')
# df8 = pd.read_excel('H ê´€ê´‘ì—¬ê°€ì˜¤ë½(4,949).xlsx', sheet_name='Sheet1')
df = pd.read_csv('kor_Pair_test.csv', sep = ',', encoding= 'utf-8')
df2 = pd.read_csv('kor_pair_train.csv', sep = ',', encoding= 'utf-8')
df3 = pd.read_csv('./csv/movie.csv', sep = ',', encoding= 'utf-8')

input_file='corpus2.txt'


with open(input_file, 'a+', encoding='utf-8') as f:
    for index, row in df3.iterrows():
        f.write('{}\n'.format(row['review']))
  
        
      
# result = pd.concat([df1, df2])
# result2 = pd.concat([df3, df4])
# result3 = pd.concat([df5, df6])
# result4 = pd.concat([df7, df8])

# test1 = pd.concat([result, result2])
# test2 = pd.concat([result3, result4])

# data = pd.concat([test1,test2])

# tagger =Mecab()

# print(df.head(20))

# text1 = 'ê°ì‚¬í•©ë‹ˆë‹¤ ì•ìœ¼ë¡œë„ ì˜ë¶€íƒë“œë ¤ìš” í’ì„±í•œí† í•‘ ë§›ë‚œí”¼ìë¡œ ë³´ë‹µí•˜ê² ìŠµë‹ˆë‹¤'
# text2 = 'ë§›ìˆê²Œ ì˜ ë¨¹ì—ˆìŠµë‹ˆë‹¤~'
# text3 = 'ë§ˆì‹œì¨íš¨!!!ë–¡ë³¶ì´ë„ì¢‹ì•„ìš”'
# text4 = 'ë¶ˆê³ ê¸°ëŠ” ì²˜ìŒ ì‹œì¼œë´¤ëŠ”ë° ìƒìƒ ê·¸ì´ìƒ....'
# text5 = 'ëƒ ëƒ ~ë„ˆë¬´ ë§›ìˆì–´ìš©^^ ë˜ ì‹œì¼œë¨¹ì–´ìš”ë„˜ë‚˜ë§›ìˆë„¤ì—¬í”¼ì§œë¡œë¤ì™œì¸ê¸°ê°€ì‡ëŠ”ì§€ì•Œê²Ÿë‘ ì›í”½ì˜ˆì•½ì„íˆì¿„'
# text6 = 'ì˜ë“±í¬í”¼ìì¤‘ ì´ì°Œë°©'
# text7 = 'ã…‹ã…‹ã…‹ã…‹ íŒŒì¸ì• í”Œ ë‹¹ì—° ì¶”ê°€í•œì¤„ì•Œê³  ì‹¤ìˆ˜í–ˆë„¤ìš”ì£„ì†¡ì—¼~~ğŸ¤£ğŸ¤£ì˜¤ëŠ˜ë„ ë§›ë‚˜ê²Œ ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤^^ğŸ¥°ìƒëŸ¬ë“œê°€ ìƒê°ë³´ë‹¤ í‘¸ì§í•˜ê²Œ ì™”ë„¤ìš”ğŸ¥°'
# print(tagger.pos(text1))
# print(tagger.pos(text2))
# print(tagger.pos(text3))
# print(tagger.pos(text4))
# print(tagger.pos(text5))
# print(tagger.pos(text6))
# print(tagger.pos(text7))

# ë„ì–´ì“°ê¸°
# [('ê°ì‚¬', 'NNG'), ('í•©ë‹ˆë‹¤', 'XSV+EF'), ('ì•', 'NNG'), ('ìœ¼ë¡œ', 'JKB'), ('ë„', 'JX'), ('ì˜', 'MAG'), ('ë¶€íƒ', 'NNG'), ('ë“œë ¤ìš”', 'VV+EC'), ('í’ì„±', 'XR'), ('í•œ', 'XSA+ETM'), ('í† í•‘', 'NNG'), ('ë§›ë‚œ', 'VA+ETM'), ('í”¼ì', 'NNG'), ('ë¡œ', 'JKB'), ('ë³´ë‹µ', 'NNG'), ('í•˜', 'XSV'), ('ê² ', 'EP'), ('ìŠµë‹ˆë‹¤', 'EF')]
# [('ë§›ìˆ', 'VA'), ('ê²Œ', 'EC'), ('ì˜', 'MAG'), ('ë¨¹', 'VV'), ('ì—ˆ', 'EP'), ('ìŠµë‹ˆë‹¤', 'EF'), ('~', 'SY')]
# [('ë§ˆì‹œ', 'NNG'), ('ì¨', 'VV+EC'), ('íš¨', 'NNG'), ('!', 'SF'), ('!!', 'SY'), ('ë–¡ë³¶ì´', 'NNG'), ('ë„', 'JX'), ('ì¢‹', 'VA'), ('ì•„ìš”', 'EC')]
# [('ë¶ˆê³ ê¸°', 'NNG'), ('ëŠ”', 'JX'), ('ì²˜ìŒ', 'NNG'), ('ì‹œì¼œ', 'XSV+EC'), ('ë´¤', 'VX+EP'), ('ëŠ”ë°', 'EC'), ('ìƒìƒ', 'NNG'), ('ê·¸', 'MM'), ('ì´ìƒ', 'NNG'), ('.', 'SF'), ('...', 'SY')]
# [('ëƒ ëƒ ', 'MAG'), ('~', 'SY'), ('ë„ˆë¬´', 'MAG'), ('ë§›ìˆ', 'VA'), ('ì–´ìš©', 'EC'), ('^^', 'SY'), ('ë˜', 'MAG'), ('ì‹œì¼œ', 'VV+EC'), ('ë¨¹', 'VV'), ('ì–´ìš”', 'EF'), ('ë„˜', 'VV'), ('ë‚˜', 'EC'), ('ë§›ìˆ', 'VA'), ('ë„¤', 'EF'), ('ì—¬í”¼', 'NNG'), ('ì§œ', 'XSN'), ('ë¡œ', 'JKB'), ('ë¤', 'NNG'), ('ì™œ', 'MAG'), ('ì¸ê¸°', 'NNG'), ('ê°€', 'JKS'), ('ì‡', 'VV'), ('ëŠ”ì§€
# ', 'EC'), ('ì•Œ', 'VV'), ('ê²Ÿ', 'EC'), ('ë‘ ', 'VX+ETN'), ('ì›', 'NNG'), ('í”½', 'NNG'), ('ì˜ˆì•½', 'NNG'), ('ì„', 'VCP+ETN'), ('íˆì¿„', 'UNKNOWN')]
# [('ì˜ë“±í¬', 'NNP'), ('í”¼ì', 'NNG'), ('ì¤‘', 'NNB'), ('ì´', 'JKS'), ('ì°Œ', 'MAG'), ('ë°©', 'NNG')]
# [('ã…‹ã…‹', 'IC'), ('ã…‹ã…‹', 'IC'), ('íŒŒì¸ì• í”Œ', 'NNG'), ('ë‹¹ì—°', 'NNG'), ('ì¶”ê°€', 'NNG'), ('í•œ', 'XSA+ETM'), ('ì¤„', 'NNB'), ('ì•Œ', 'VV'), ('ê³ ', 'EC'), ('ì‹¤ìˆ˜', 'NNG'), ('í–ˆ', 'XSV+EP'), ('ë„¤ìš”', 'EF'), ('ì£„', 'NNG'), ('ì†¡ì—¼', 'NNP'), ('~~', 'SY'), ('ï¿½ï¿½ğŸ¤£ğŸ¤£', 'SY'),(ì˜¤ì˜¤ëŠ˜ëŠ˜', 'NNG'),('ë„ë„', 'JX'),(ë§›ë§›ë‚˜ë‚˜', 'VA'),('ê²Œê²Œ', 'EC'),('ì˜ì˜', 'MAG'),('
#  'VV'), ('ê² ', 'EP'), ('ìŠµë‹ˆë‹¤', 'EF'), ('^^', 'SY'), ('ğŸ¥°', 'SY'), ('ìƒëŸ¬ë“œ', 'NNG'), ('ê°€', 'JKS'), ('ìƒê°',  'NN'), ('ë³´ë‹¤', 'JKB'), ('í‘¸ì§', 'XR'), ('í•˜', 'XSA'), ('ê²Œ', 'EC'), ('ì™”', 'VX+EP'), ('ë„¤ìš”', 'EF'), ('ğŸ¥°', 'SY ')]  

# ===========================================Sentencepiece =====================================

#print(df1.head(20))

# input_file = './csv/corpus.txt'

# # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë§Œë“¤ê¸° -> setencepieceëŠ” txtë§Œ ë°›ìŒ
# with open(input_file, 'a+', encoding='utf-8') as f:
#     for index, row in data.iterrows():
#         f.write('{}\n'.format(row['SENTENCE']))
#         # if row['CATEGORY'] == 'ë°°ë‹¬ìŒì‹ì ':
#         #     f.write ('{}\n'.format(row['SENTENCE']))


# parameter = '--input={} --model_prefix={} --vocab_size={} --user_defined_symbols={}'

# input_file = './csv/corpus.txt'
# vocab_size = 12510 #3,2000ì´ ê°€ì¥ ì¢‹ìœ¼ë‚˜ í•  ìˆ˜ê°€ ì—†ìŒã… 
# prefix = 'test' #'bert_kor'
# user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK]'
# model_type = 'bpe'

# cmd = parameter.format(input_file, prefix, vocab_size, user_defined_symbols)

# spm.SentencePieceTrainer.Train(cmd)


# sp = spm.SentencePieceProcessor()
# sp.Load('{}.model'.format(prefix)) 
# token = sp.EncodeAsPieces(text1) #['â–ê°ì‚¬í•©ë‹ˆë‹¤', 'â–', 'ì•', 'ìœ¼ë¡œ', 'ë„', 'â–ì˜', 'ë¶€', 'íƒ', 'ë“œë ¤ìš”', 'â–', 'í’', 'ì„±', 'í•œ', 'í† í•‘', 'â–ë§›', 'ë‚œ', 'í”¼ì', 'ë¡œ', 'â–ë³´', 'ë‹µ', 'í•˜ê² ìŠµë‹ˆë‹¤']
# token2 = sp.EncodeAsPieces(text2) #['â–ë§›ìˆ', 'ê²Œ', 'â–ì˜', 'â–ë¨¹', 'ì—ˆ', 'ìŠµë‹ˆë‹¤', '~']
# token3 = sp.EncodeAsPieces(text3) #['â–ë§ˆ', 'ì‹œ', 'ì¨', 'íš¨', '!', '!', '!', 'ë–¡', 'ë³¶', 'ì´', 'ë„', 'ì¢‹', 'ì•„ìš”']
# token4 = sp.EncodeAsPieces(text4) #['â–ë¶ˆê³ ê¸°', 'ëŠ”', 'â–', 'ì²˜', 'ìŒ', 'â–ì‹œì¼œ', 'ë´¤', 'ëŠ”ë°', 'â–ìƒ', 'ìƒ', 'â–ê·¸', 'ì´ìƒ', '.', '.', '.', '.']
# token5 = sp.EncodeAsPieces(text5) #['â–', 'ëƒ ëƒ ', '~', 'ë„ˆ', 'ë¬´', 'â–ë§›ìˆ', 'ì–´', 'ìš©', '^^', 'â–', 'ë˜', 'â–ì‹œì¼œ', 'ë¨¹', 'ì–´ìš”', 'ë„˜', 'ë‚˜', 'ë§›', 'ìˆ', 'ë„¤', 'ì—¬', 'í”¼', 'ì§œ', 'ë¡œ', 'ë¤', 'ì™œ', 'ì¸', 'ê¸°ê°€', 'ì‡', 'ëŠ”', 'ì§€', 'ì•Œ', 'ê²Ÿë‘ ', 'ì›', 'í”½', 'ì˜ˆ', 'ì•½', 'ì„', 'íˆì¿„']
# token6 = sp.EncodeAsPieces(text6) #['â–', 'ì˜', 'ë“±', 'í¬', 'í”¼ì', 'ì¤‘', 'â–ì´', 'ì°Œ', 'ë°©']
# token7 = sp.EncodeAsPieces(text7) 

# ['â–ê°ì‚¬í•©ë‹ˆë‹¤', 'â–ì•', 'ìœ¼ë¡œ', 'ë„', 'â–ì˜', 'ë¶€íƒë“œë ¤ìš”', 'â–', 'í’', 'ì„±', 'í•œ', 'í† í•‘', 'â–ë§›', 'ë‚œ', 'í”¼ì', 'ë¡œ', 'â–ë³´', 'ë‹µ', 'í•˜ê² ìŠµë‹ˆë‹¤']
# ['â–ë§›ìˆê²Œ', 'â–ì˜', 'â–ë¨¹ì—ˆ', 'ìŠµë‹ˆë‹¤', '~']
# ['â–ë§ˆì‹œ', 'ì¨', 'íš¨', '!', '!', '!', 'ë–¡ë³¶ì´', 'ë„', 'ì¢‹', 'ì•„ìš”']
# ['â–ë¶ˆê³ ê¸°', 'ëŠ”', 'â–ì²˜ìŒ', 'â–ì‹œì¼œ', 'ë´¤', 'ëŠ”ë°', 'â–ìƒ', 'ìƒ', 'â–ê·¸', 'ì´ìƒ', '.', '.', '.', '.']
# ['â–', 'ëƒ ëƒ ', '~', 'ë„ˆ', 'ë¬´', 'â–ë§›ìˆì–´', 'ìš©', '^^', 'â–', 'ë˜', 'â–ì‹œì¼œ', 'ë¨¹ì–´ìš”', 'ë„˜', 'ë‚˜', 'ë§›', 'ìˆ', 'ë„¤', 'ì—¬', 'í”¼', 'ì§œ', 'ë¡œ', 'ë¤', 'ì™œ', 'ì¸', 'ê¸°ê°€', 'ì‡', 'ëŠ”', 'ì§€', 'ì•Œ', 'ê²Ÿ', 'ë‘ ', 'ì›', 'í”½
# ', 'ì˜ˆì•½', 'ì„', 'íˆì¿„']
# ['â–ì˜', 'ë“±', 'í¬', 'í”¼ì', 'ì¤‘', 'â–ì´', 'ì°Œ', 'ë°©']

'''
['â–ê°ì‚¬í•©ë‹ˆë‹¤', 'â–ì•ìœ¼ë¡œ', 'ë„', 'â–ì˜', 'ë¶€íƒë“œë ¤ìš”', 'â–í’ì„±', 'í•œ', 'í† í•‘', 'â–ë§›', 'ë‚œ', 'í”¼ì', 'ë¡œ', 'â–ë³´', 'ë‹µ', 'í•˜ê² ìŠµë‹ˆë‹¤']
['â–ë§›ìˆê²Œ', 'â–ì˜', 'â–ë¨¹ì—ˆ', 'ìŠµë‹ˆë‹¤', '~']
['â–ë§ˆì‹œ', 'ì¨', 'íš¨', '!', '!', '!', 'ë–¡ë³¶ì´', 'ë„', 'ì¢‹ì•„ìš”']
['â–ë¶ˆê³ ê¸°', 'ëŠ”', 'â–ì²˜ìŒ', 'â–ì‹œì¼œ', 'ë´¤ëŠ”ë°', 'â–ìƒ', 'ìƒ', 'â–ê·¸', 'ì´ìƒ', '.', '.', '.', '.']
['â–', 'ëƒ ëƒ ', '~', 'ë„ˆ', 'ë¬´', 'â–ë§›ìˆì–´', 'ìš©', '^^', 'â–ë˜', 'â–ì‹œì¼œë¨¹', 'ì–´ìš”', 'ë„˜', 'ë‚˜', 'ë§›', 'ìˆ', 'ë„¤', 'ì—¬', 'í”¼', 'ì§œ', 'ë¡œ', 'ë¤', 'ì™œ', 'ì¸', 'ê¸°ê°€', 'ì‡', 'ëŠ”', 'ì§€', 'ì•Œ', 'ê²Ÿ', 'ë‘ ', 'ì›', 'í”½', 'ì˜ˆì•½', 'ì„', 'íˆì¿„']
['â–ì˜', 'ë“±', 'í¬', 'í”¼ì', 'ì¤‘', 'â–ì´', 'ì°Œ', 'ë°©']
['â–', 'á„á„á„á„', 'â–íŒŒì¸ì• í”Œ', 'â–ë‹¹ì—°', 'â–ì¶”ê°€', 'í•œ', 'ì¤„', 'ì•Œ', 'ê³ ', 'â–ì‹¤ìˆ˜', 'í–ˆ', 'ë„¤ìš”', 'ì£„', 'ì†¡', 'ì—¼', '~', '~', 'ï¿½ï¿½ğŸ¤£ğŸ¤£' ì˜¤ì˜¤ëŠ˜ëŠ˜' 'ë„ë„','â–ë§›ë§›' 'ë‚˜ë‚˜' 'ê²Œê²Œ','â–ì˜ì˜','â–ë¨¹ë¨¹' ê² ìŠµë‹ˆë‹ˆë‹¤ë‹¤', '^^ğŸ¥°,'ìƒëŸ¬ëŸ¬ë“œ ê°', 'ë³´ë‹¤', 'â–í‘¸', 'ì§', 'í•˜ê²Œ', 'â–ì™”', 'ë„¤ìš”', 'ğŸ¥°']
'ê°€', 'â–ìƒê°', 'ë³´ë‹¤', 'â–í‘¸', 'ì§', 'í•˜ê²Œ', 'â–ì™”', 'ë„¤ìš”', 'ğŸ¥°']




['â–ê°ì‚¬í•©ë‹ˆë‹¤', 'â–ì•ìœ¼ë¡œ', 'ë„', 'â–ì˜', 'ë¶€íƒë“œë ¤ìš”', 'â–í’ì„±', 'í•œ', 'í† í•‘', 'â–ë§›', 'ë‚œ', 'í”¼ì', 'ë¡œ', 'â–ë³´', 'ë‹µ', 'í•˜ê² ìŠµë‹ˆë‹¤']
['â–ë§›ìˆê²Œ', 'â–ì˜', 'â–ë¨¹', 'ì—ˆ', 'ìŠµë‹ˆë‹¤', '~']
['â–ë§ˆì‹œ', 'ì¨', 'íš¨', '!', '!', '!', 'ë–¡ë³¶ì´', 'ë„', 'ì¢‹', 'ì•„ìš”']
['â–ë¶ˆê³ ê¸°', 'ëŠ”', 'â–ì²˜ìŒ', 'â–ì‹œì¼œ', 'ë´¤ëŠ”ë°', 'â–ìƒ', 'ìƒ', 'â–ê·¸', 'ì´ìƒ', '.', '.', '.', '.']
['â–', 'ëƒ ëƒ ', '~', 'ë„ˆ', 'ë¬´', 'â–ë§›ìˆì–´', 'ìš©', '^^', 'â–ë˜', 'â–', 'ì‹œì¼œë¨¹ì–´', 'ìš”', 'ë„˜', 'ë‚˜', 'ë§›', 'ìˆ', 'ë„¤', 'ì—¬', 'í”¼', 'ì§œ', 'ë¡œ', 'ë¤', 'ì™œ', 'ì¸', 'ê¸°ê°€', 'ì‡', 'ëŠ”', 'ì§€', 'ì•Œ', 'ê²Ÿ', 'ë‘ ', 'ì›', 'í”½
', 'ì˜ˆì•½', 'ì„', 'íˆì¿„']
['â–ì˜', 'ë“±', 'í¬', 'í”¼ì', 'ì¤‘', 'â–ì´', 'ì°Œ', 'ë°©']
['â–', 'á„á„á„á„', 'â–íŒŒì¸ì• í”Œ', 'â–ë‹¹', 'ì—°', 'â–ì¶”ê°€', 'í•œ', 'ì¤„', 'ì•Œ', 'ê³ ', 'â–ì‹¤', 'ìˆ˜', 'í–ˆ', 'ë„¤ìš”', 'ì£„', 'ì†¡', 'ì—¼', '~', '~', 'ï¿½ï¿½ğŸ¤£ğŸ¤£' 'ì˜¤ì˜¤' 'ëŠ˜ëŠ˜' 'ë„ë„','â–ë§›ë§›' 'ë‚˜ë‚˜' 'ê²Œê²Œ','â–ì˜ì˜','â–ë¨¹ë¨¹' ê² ìŠµë‹ˆë‹ˆë‹¤ë‹¤', '^', 'ê°€', 'â–ìƒê°', 'ë³´ë‹¤', 'â–', 'í‘¸', 'ì§', 'í•˜ê²Œ', 'â–ì™”', 'ë„¤ìš”', 'ğŸ¥°']
, 'ìƒëŸ¬ë“œ', 'ê°€', 'â–ìƒê°', 'ë³´ë‹¤', 'â–', 'í‘¸', 'ì§', 'í•˜ê²Œ', 'â–ì™”', 'ë„¤ìš”', 'ğŸ¥°']


'''

# print(token)
# print(token2)
# print(token3)
# print(token4)
# print(token5)
# print(token6)
# print(token7)

'''
input_file : ë‚´ í•œêµ­ì–´ ë°ì´í„° ê²½ë¡œë¥¼ ì§€ì •í•œë‹¤.
vocab_size : BPEì˜ ë‹¨ì–´ìˆ˜ë¥¼ ì–¼ë§ˆë¡œ í•  ê²ƒì¸ê°€ ì´ë‹¤. ë„ˆë¬´ ì ìœ¼ë©´ í•œ ê¸€ì ë‹¨ìœ„ë¡œ ìª¼ê°œì§€ëŠ” ê²½í–¥ì´ ìˆê³ , ë„ˆë¬´ ë§ìœ¼ë©´ ì“¸ë°ì—†ëŠ” ë‹¨ì–´ë“¤ì´ ë§Œë“¤ì–´ì§„ë‹¤. ì£¼ë¡œ 3,2000ì´ ê°€ì¥ ì¢‹ë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤.
model_name : ì €ì¥í•  ì´ë¦„ì´ë‹¤. í•™ìŠµí•˜ê³  ë‚˜ë©´ <model_name>.model, <model_name>.vocab 2ê°œì˜ íŒŒì¼ì´ ë§Œë“¤ì–´ì§„ë‹¤.
model_type : bpe, unigram ë“±ì´ ìˆëŠ”ë° ë‘ê°€ì§€ë¥¼ ëª¨ë‘ ì‚¬ìš©í•´ ë³´ê³  ì„±ëŠ¥ì´ ì¢‹ì€ ê±°ë¡œ ê³ ë¥´ë„ë¡ í•˜ì. 
character_coverage : ëª¨ë“  ë‹¨ì–´ë¥¼ ì»¤ë²„í• ê²ƒì¸ê°€, ë„ˆë¬´ í¬ê·€í•œ ë‹¨ì–´ëŠ” ëº„ ê²ƒì¸ê°€ ì´ë‹¤. í•™ìŠµ ì½”í¼ìŠ¤ê°€ ëŒ€ìš©ëŸ‰ì´ë¼ë©´ ë³´í†µ 0.9995ë¡œ ì‚¬ìš©í•˜ë©´ ëœë‹¤. ê·¸ëŸ°ë° ì½”í¼ìŠ¤ê°€ ì‘ë‹¤ë©´ 1.0ìœ¼ë¡œ ì§€ì •í•˜ì. ê·¸ëŸ¼ [UNK]ê°€ ì—†ë‹¤.
user_defined_symbols : BPEë¡œ ìƒì„±ëœ ë‹¨ì–´ ì™¸ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì‚¬ìš©í•  íŠ¹ìˆ˜ë¬¸ìë“¤ì„ ì§€ì •í•œë‹¤.
'''
# input_file = './csv/corpus.txt'
# vocab_size = 12524
# model_name = 'model_sentencepiece/sentencepiece_tokenizer_kor_%d' % (vocab_size)
# model_type = 'bpe'
# character_coverage  = 1.0  # 0.9995
# user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99]'

# input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'
# cmd1 = input_argument%(input_file, model_name, vocab_size,user_defined_symbols, model_type, character_coverage)

# spm.SentencePieceTrainer.Train(cmd1)
# print('train done')
 

# sp = spm.SentencePieceProcessor()
# sp.Load('{}.model'.format(model_name))
# sentencepiece_tokenizer = sp.encode
# token = sentencepiece_tokenizer(text1, out_type=str)

# print(token)